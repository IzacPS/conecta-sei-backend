"""
ProcessExtractor - Mﾃｳdulo de extraﾃｧﾃ｣o de processos (Sprint 2.1)

Este mﾃｳdulo implementa o pipeline de extraﾃｧﾃ｣o de processos em 2 etapas:
1. Stage 1: Descoberta rﾃ｡pida de processos (tabela principal)
2. Stage 2+3 (merged): Validaﾃｧﾃ｣o de links + Extraﾃｧﾃ｣o de documentos (navegaﾃｧﾃ｣o individual)

Caracterﾃｭsticas:
- Multithreading com ThreadPoolExecutor (5-10 workers)
- Detecﾃｧﾃ｣o de novos processos e documentos
- Notificaﾃｧﾃｵes para ambos os casos
- Integraﾃｧﾃ｣o com PostgreSQL + ParadeDB via repositories
- Uso do InstitutionService para seleﾃｧﾃ｣o de scraper correto
- Abstraﾃｧﾃ｣o via SEIScraperBase (plugin system)
"""

import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from playwright.sync_api import Page, Browser

from app.core.services.institution_service import InstitutionService
from app.database.repositories.process_repository import ProcessRepository
from app.database.repositories.institution_repository import InstitutionRepository
from app.scrapers.base import SEIScraperBase
from app.utils import email_service as email_svc

logger = logging.getLogger(__name__)


class ProcessExtractor:
    """
    Extrator de processos do SEI com pipeline otimizado.

    Pipeline:
    1. discover_process_list() - Lista processos da tabela principal (rﾃ｡pido)
    2. process_worker() - Para cada processo: valida link + extrai documentos (paralelo)
    3. Detecta novos processos e documentos
    4. Envia notificaﾃｧﾃｵes apropriadas

    Utiliza o plugin system de scrapers via InstitutionService para garantir
    compatibilidade com diferentes versﾃｵes do SEI.
    """

    def __init__(
        self,
        process_repo: ProcessRepository,
        institution_repo: InstitutionRepository,
        institution_service: InstitutionService,
        max_workers: int = 5
    ):
        """
        Inicializa o ProcessExtractor.

        Args:
            process_repo: Repositﾃｳrio de processos
            institution_repo: Repositﾃｳrio de instituiﾃｧﾃｵes
            institution_service: Serviﾃｧo de gerenciamento de instituiﾃｧﾃｵes
            max_workers: Nﾃｺmero de threads para processamento paralelo
        """
        self.process_repo = process_repo
        self.institution_repo = institution_repo
        self.institution_service = institution_service
        self.max_workers = max_workers

        # Caches para otimizaﾃｧﾃ｣o
        self._existing_processes: Dict[str, dict] = {}
        self._new_processes: List[str] = []
        self._new_documents: Dict[str, List[str]] = {}

    def discover_process_list(
        self,
        page: Page,
        scraper: SEIScraperBase,
        institution_id: str
    ) -> Dict[str, dict]:
        """
        Stage 1: Descoberta rﾃ｡pida de processos da tabela principal.

        Esta etapa ﾃｩ ultra-otimizada:
        - Delega para scraper.extract_process_list() (versﾃ｣o-especﾃｭfico)
        - Apenas lﾃｪ a tabela HTML da pﾃ｡gina principal
        - Extrai nﾃｺmeros de processos e links
        - Nﾃグ navega para processos individuais
        - Nﾃグ coleta dados de autoridade (serﾃ｡ feito em Stage 2+3)

        Args:
            page: Pﾃ｡gina Playwright jﾃ｡ logada
            scraper: Scraper especﾃｭfico da versﾃ｣o SEI
            institution_id: ID da instituiﾃｧﾃ｣o

        Returns:
            Dict com {numero_processo: {processo_data}}
        """
        logger.info("=== STAGE 1: Descoberta de processos ===")

        try:
            # Navega para pﾃ｡gina de listagem de processos
            list_url = scraper.get_process_list_url()
            page.goto(list_url, timeout=30000)
            scraper.wait_for_page_load(page)

            # Extrai processos usando scraper especﾃｭfico da versﾃ｣o
            processes = scraper.extract_process_list(page)

            # Adiciona metadados
            for process_number in processes:
                processes[process_number]["institution_id"] = institution_id
                processes[process_number]["discovered_at"] = datetime.now().isoformat()

            logger.info(f"Stage 1 concluﾃｭdo: {len(processes)} processos descobertos")
            return processes

        except Exception as e:
            logger.error(f"Erro na descoberta de processos: {e}")
            raise

    def _compare_processes(self, old_processes: Set[str], new_processes: Set[str]) -> List[str]:
        """
        Compara processos antigos e novos para identificar novos processos.

        Args:
            old_processes: Set de nﾃｺmeros de processos jﾃ｡ existentes no banco
            new_processes: Set de nﾃｺmeros de processos descobertos no SEI

        Returns:
            Lista de nﾃｺmeros de processos novos
        """
        if not old_processes:
            return list(new_processes)

        new = new_processes - old_processes
        return sorted(list(new))

    def process_worker(
        self,
        browser: Browser,
        process_number: str,
        process_data: dict,
        scraper: SEIScraperBase,
        institution_id: str
    ) -> Optional[dict]:
        """
        Stage 2+3 (merged): Valida link + Extrai documentos em uma ﾃｺnica navegaﾃｧﾃ｣o.

        Esta funﾃｧﾃ｣o ﾃｩ executada em paralelo por mﾃｺltiplos workers.

        Fluxo:
        1. Abre nova pﾃ｡gina no browser
        2. Navega para o link do processo (via scraper.validate_link)
        3. Valida se o link ﾃｩ vﾃ｡lido (Stage 2)
        4. Determina tipo de acesso: "integral" ou "parcial" (Stage 2)
        5. Coleta autoridade se estiver faltando (Stage 2)
        6. Se should_process_documents() == True: extrai documentos (Stage 3)
        7. Detecta novos documentos (Stage 3)

        Args:
            browser: Browser Playwright (thread-safe)
            process_number: Nﾃｺmero do processo
            process_data: Dados do processo (links, etc)
            scraper: Scraper especﾃｭfico da versﾃ｣o SEI
            institution_id: ID da instituiﾃｧﾃ｣o

        Returns:
            Dict com dados atualizados do processo, ou None se erro
        """
        page = None
        try:
            # Cria nova pﾃ｡gina (thread-safe)
            page = browser.new_page()

            logger.info(f"[{process_number}] Iniciando processamento (Stage 2+3)")

            # Extrai melhor link disponﾃｭvel
            links = process_data.get("links", {})
            if not links:
                logger.error(f"[{process_number}] Nenhum link encontrado")
                return None

            # Pega primeiro link disponﾃｭvel (Stage 1 jﾃ｡ encontrou)
            link = list(links.keys())[0]

            # === STAGE 2: Validaﾃｧﾃ｣o de link ===
            logger.info(f"[{process_number}] Validando link...")

            validation_result = scraper.validate_link(page, link)

            if not validation_result.get("valid"):
                logger.warning(f"[{process_number}] Link invﾃ｡lido: {validation_result.get('error', 'Desconhecido')}")
                return {
                    "process_number": process_number,
                    "link_valid": False,
                    "access_type": None,
                    "authority": None,
                    "documents": {},
                    "error": validation_result.get("error")
                }

            # Link vﾃ｡lido - extrai informaﾃｧﾃｵes
            access_type = validation_result.get("tipo_acesso", "desconhecido")
            authority = validation_result.get("autoridade")

            logger.info(f"[{process_number}] Link vﾃ｡lido - Tipo de acesso: {access_type}")

            # === STAGE 3: Extraﾃｧﾃ｣o de documentos ===

            documents = {}

            # Verifica se deve processar documentos
            if self._should_process_documents(access_type, process_data):
                logger.info(f"[{process_number}] Extraindo documentos...")

                try:
                    documents = scraper.extract_documents(page)
                    logger.info(f"[{process_number}] {len(documents)} documentos extraﾃｭdos")
                except Exception as e:
                    logger.error(f"[{process_number}] Erro ao extrair documentos: {e}")
            else:
                logger.info(f"[{process_number}] Pula extraﾃｧﾃ｣o de documentos (should_process_documents=False)")

            return {
                "process_number": process_number,
                "link": link,
                "link_valid": True,
                "access_type": access_type,
                "authority": authority,
                "documents": documents,
                "updated_at": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"[{process_number}] Erro no processamento: {e}")
            return None

        finally:
            if page:
                page.close()

    def _should_process_documents(self, access_type: str, process_data: dict) -> bool:
        """
        Determina se deve processar documentos do processo.

        Lﾃｳgica (replicada do legado utils.py:93-115):
        - Se access_type == "integral": sempre processar
        - Se access_type == "parcial":
            - Se status_categoria == "pendente": Nﾃグ processar
            - Se categoria == "restrito": processar
            - Caso contrﾃ｡rio: Nﾃグ processar

        Args:
            access_type: "integral" ou "parcial"
            process_data: Dados do processo com categoria e status

        Returns:
            True se deve processar documentos, False caso contrﾃ｡rio
        """
        if access_type == "integral":
            return True

        # Acesso parcial: sﾃｳ processa em casos especﾃｭficos
        status_categoria = process_data.get("status_categoria")
        categoria = process_data.get("categoria")

        if status_categoria == "pendente":
            return False

        if categoria == "restrito":
            return True

        return False

    def _detect_new_documents(
        self,
        process_number: str,
        existing_docs: Dict[str, dict],
        new_docs: Dict[str, dict]
    ) -> List[str]:
        """
        Detecta novos documentos comparando com documentos existentes.

        Args:
            process_number: Nﾃｺmero do processo
            existing_docs: Documentos jﾃ｡ existentes no banco
            new_docs: Documentos extraﾃｭdos agora do SEI

        Returns:
            Lista de nﾃｺmeros de documentos novos
        """
        new_doc_numbers = []

        for doc_number in new_docs.keys():
            # Documento ﾃｩ novo se nﾃ｣o existe no banco
            if doc_number not in existing_docs:
                new_doc_numbers.append(doc_number)
                logger.info(f"[{process_number}] Novo documento detectado: {doc_number}")

        return new_doc_numbers

    def run_extraction(
        self,
        browser: Browser,
        page: Page,
        institution_id: str
    ) -> Tuple[int, int, int]:
        """
        Executa o pipeline completo de extraﾃｧﾃ｣o.

        Fluxo:
        1. Obtﾃｩm scraper correto via InstitutionService
        2. Stage 1: discover_process_list() - lista processos (rﾃ｡pido, sequencial)
        3. Carrega processos existentes do banco
        4. Detecta novos processos
        5. Stage 2+3: process_worker() - processa cada processo (paralelo, multithreading)
        6. Salva resultados no banco
        7. Detecta novos documentos
        8. Envia notificaﾃｧﾃｵes

        Args:
            browser: Browser Playwright
            page: Pﾃ｡gina Playwright jﾃ｡ logada
            institution_id: ID da instituiﾃｧﾃ｣o

        Returns:
            Tupla (total_processos, novos_processos, novos_documentos)
        """
        logger.info("=== INICIANDO PIPELINE DE EXTRAﾃﾃグ ===")

        # === Obtﾃｩm scraper correto ===
        scraper = self.institution_service.get_scraper(institution_id)
        logger.info(f"Usando scraper: {scraper.get_scraper_name()}")

        # === STAGE 1: Descoberta de processos ===
        discovered_processes = self.discover_process_list(page, scraper, institution_id)
        total_processes = len(discovered_processes)

        if total_processes == 0:
            logger.warning("Nenhum processo descoberto. Encerrando.")
            return (0, 0, 0)

        # === Carrega processos existentes do banco ===
        existing_processes_data = self.process_repo.get_all_by_institution(institution_id)
        existing_process_numbers = {p["process_number"] for p in existing_processes_data}
        self._existing_processes = {p["process_number"]: p for p in existing_processes_data}

        logger.info(f"Processos existentes no banco: {len(existing_process_numbers)}")

        # === Detecta novos processos ===
        discovered_process_numbers = set(discovered_processes.keys())
        self._new_processes = self._compare_processes(existing_process_numbers, discovered_process_numbers)

        logger.info(f"Novos processos detectados: {len(self._new_processes)}")
        if self._new_processes:
            logger.info(f"Lista: {', '.join(self._new_processes[:10])}{'...' if len(self._new_processes) > 10 else ''}")

        # === STAGE 2+3: Processamento paralelo ===
        logger.info(f"=== STAGE 2+3: Processamento paralelo ({self.max_workers} workers) ===")

        processed_count = 0
        self._new_documents = {}

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submete todos os processos para processamento
            future_to_process = {
                executor.submit(
                    self.process_worker,
                    browser,
                    process_number,
                    process_data,
                    scraper,
                    institution_id
                ): process_number
                for process_number, process_data in discovered_processes.items()
            }

            # Processa resultados conforme completam
            for future in as_completed(future_to_process):
                process_number = future_to_process[future]

                try:
                    result = future.result()

                    if result:
                        processed_count += 1

                        # Salva resultado no banco
                        self._save_process_result(result)

                        # Detecta novos documentos
                        if result.get("documents"):
                            existing_docs = self._existing_processes.get(process_number, {}).get("documents", {})
                            new_docs = self._detect_new_documents(
                                process_number,
                                existing_docs,
                                result["documents"]
                            )

                            if new_docs:
                                self._new_documents[process_number] = new_docs

                        logger.info(f"Progresso: {processed_count}/{total_processes} processos")

                except Exception as e:
                    logger.error(f"Erro ao processar resultado de {process_number}: {e}")

        # === Envia notificaﾃｧﾃｵes ===
        total_new_docs = sum(len(docs) for docs in self._new_documents.values())

        logger.info(f"\n=== RESUMO DA EXTRAﾃﾃグ ===")
        logger.info(f"Total de processos: {total_processes}")
        logger.info(f"Novos processos: {len(self._new_processes)}")
        logger.info(f"Processos com novos documentos: {len(self._new_documents)}")
        logger.info(f"Total de novos documentos: {total_new_docs}")

        # Notificaﾃｧﾃｵes (usa discovered_processes para montar links nos emails)
        self._send_notifications(discovered_processes)

        return (total_processes, len(self._new_processes), total_new_docs)

    def _save_process_result(self, result: dict) -> None:
        """
        Salva resultado do processamento no banco.

        Args:
            result: Dict com dados do processo processado
        """
        process_number = result["process_number"]

        try:
            # Verifica se processo jﾃ｡ existe
            existing = self.process_repo.get_by_number(process_number)

            if existing:
                # Atualiza processo existente
                self.process_repo.update(
                    process_number,
                    {
                        "link_valid": result["link_valid"],
                        "access_type": result["access_type"],
                        "authority": result.get("authority"),
                        "documents": result.get("documents", {}),
                        "updated_at": result["updated_at"]
                    }
                )
            else:
                # Cria novo processo
                self.process_repo.create({
                    "process_number": process_number,
                    "link_valid": result["link_valid"],
                    "access_type": result["access_type"],
                    "authority": result.get("authority"),
                    "documents": result.get("documents", {}),
                    "created_at": datetime.now().isoformat(),
                    "updated_at": result["updated_at"]
                })

        except Exception as e:
            logger.error(f"Erro ao salvar processo {process_number}: {e}")

    def _send_notifications(self, discovered_processes: Dict[str, dict]) -> None:
        """
        Envia notificaﾃｧﾃｵes por email sobre novos processos e documentos.

        Mantﾃｩm comportamento do legado:
        - Notificaﾃｧﾃ｣o de novos processos (se houver)
        - Notificaﾃｧﾃ｣o de novos documentos (se houver, independente se processo ﾃｩ novo)

        Nﾃグ filtra documentos de processos novos - ambas notificaﾃｧﾃｵes sﾃ｣o enviadas.
        """
        if self._new_processes:
            logger.info(f"\n透 NOTIFICAﾃﾃグ: {len(self._new_processes)} novos processos")
            # Monta lista com process_number e link para o email (discovered_processes: {process_number: {links: {...}}})
            payload = []
            for p in self._new_processes:
                data = discovered_processes.get(p, {})
                link = ""
                if isinstance(data, dict):
                    links = data.get("links", {})
                    if links:
                        link = next(iter(links.keys()), "")
                payload.append({"process_number": p, "link": link})
            try:
                if email_svc.notify_new_processes(payload):
                    logger.info("Email de novos processos enviado.")
                else:
                    logger.warning("Falha ao enviar email de novos processos.")
            except Exception as e:
                logger.exception("Erro ao enviar notificaﾃｧﾃ｣o de novos processos: %s", e)

        if self._new_documents:
            total_docs = sum(len(docs) for docs in self._new_documents.values())
            logger.info(f"\n透 NOTIFICAﾃﾃグ: {total_docs} novos documentos em {len(self._new_documents)} processos")
            # Formato esperado por notify_new_documents: {processo: {apelido, documentos_por_signatario}}
            process_data = {}
            for process_number, doc_list in self._new_documents.items():
                apelido = email_svc.get_process_nickname(process_number)
                process_data[process_number] = {
                    "apelido": apelido or "",
                    "documentos_por_signatario": {"Novos documentos": doc_list},
                }
            try:
                if email_svc.notify_new_documents(process_data):
                    logger.info("Email de novos documentos enviado.")
                else:
                    logger.warning("Falha ao enviar email de novos documentos.")
            except Exception as e:
                logger.exception("Erro ao enviar notificaﾃｧﾃ｣o de novos documentos: %s", e)
